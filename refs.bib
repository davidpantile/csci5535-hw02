@inproceedings{Steinbakken,
    title = "Native-Language Identification with Attention",
    author = {Steinbakken, Stian  and
      Gamb{\"a}ck, Bj{\"o}rn},
    booktitle = "Proceedings of the 17th International Conference on Natural Language Processing (ICON)",
    month = dec,
    year = "2020",
    address = "Indian Institute of Technology Patna, Patna, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2020.icon-main.35",
    pages = "261--271",
    abstract = "The paper explores how an attention-based approach can increase performance on the task of native-language identification (NLI), i.e., to identify an author{'}s first language given information expressed in a second language. Previously, Support Vector Machines have consistently outperformed deep learning-based methods on the TOEFL11 data set, the de facto standard for evaluating NLI systems. The attention-based system BERT (Bidirectional Encoder Representations from Transformers) was first tested in isolation on the TOEFL11 data set, then used in a meta-classifier stack in combination with traditional techniques to produce an accuracy of 0.853. However, more labelled NLI data is now available, so BERT was also trained on the much larger Reddit-L2 data set, containing 50 times as many examples as previously used for English NLI, giving an accuracy of 0.902 on the Reddit-L2 in-domain test scenario, improving the state-of-the-art by 21.2 percentage points.",
}

@article{Tziafas,
  author       = {Giorgos Tziafas and
                  Konstantinos Kogkalidis and
                  Gijs Wijnholds and
                  Michael Moortgat},
  title        = {Improving {BERT} Pretraining with Syntactic Supervision},
  journal      = {CoRR},
  volume       = {abs/2104.10516},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.10516},
  eprinttype    = {arXiv},
  eprint       = {2104.10516},
  timestamp    = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-10516.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Koppel,
author = {Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir},
year = {2005},
month = {08},
pages = {624-628},
title = {Determining an author's native language by mining a text for errors},
booktitle = {Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining},
doi = {10.1145/1081870.1081947}
}

@article{translationese,
    author = {Volansky, Vered and Ordan, Noam and Wintner, Shuly},
    title = "{On the features of translationese}",
    journal = {Digital Scholarship in the Humanities},
    volume = {30},
    number = {1},
    pages = {98-118},
    year = {2013},
    month = {07},
    abstract = "{Much research in translation studies indicates that translated texts are ontologically different from original non-translated ones. Translated texts, in any language, can be considered a dialect of that language, known as ‘translationese’. Several characteristics of translationese have been proposed as universal in a series of hypotheses. In this work, we test these hypotheses using a computational methodology that is based on supervised machine learning. We define several classifiers that implement various linguistically informed features, and assess the degree to which different sets of features can distinguish between translated and original texts. We demonstrate that some feature sets are indeed good indicators of translationese, thereby corroborating some hypotheses, whereas others perform much worse (sometimes at chance level), indicating that some ‘universal’ assumptions have to be reconsidered.In memoriam: Miriam Shlesinger, 1947–2012}",
    issn = {2055-7671},
    doi = {10.1093/llc/fqt031},
    url = {https://doi.org/10.1093/llc/fqt031},
    eprint = {https://academic.oup.com/dsh/article-pdf/30/1/98/21521905/fqt031.pdf},
}




@article{Sarwar,
author = {Sarwar, Raheem and Rutherford, Attapol T. and Hassan, Saeed-Ul and Rakthanmanon, Thanawin and Nutanong, Sarana},
title = {Native Language Identification of Fluent and Advanced Non-Native Writers},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3383202},
doi = {10.1145/3383202},
abstract = {Native Language Identification (NLI) aims at identifying the native languages of authors by analyzing their text samples written in a non-native language. Most existing studies investigate this task for educational applications such as second language acquisition and require the learner corpora. This article performs NLI in a challenging context of the user-generated-content (UGC) where authors are fluent and advanced non-native speakers of a second language. Existing NLI studies with UGC (i) rely on the content-specific/social-network features and may not be generalizable to other domains and datasets, (ii) are unable to capture the variations of the language-usage-patterns within a text sample, and (iii) are not associated with any outlier handling mechanism. Moreover, since there is a sizable number of people who have acquired non-English second languages due to the economic and immigration policies, there is a need to gauge the applicability of NLI with UGC to other languages. Unlike existing solutions, we define a topic-independent feature space, which makes our solution generalizable to other domains and datasets. Based on our feature space, we present a solution that mitigates the effect of outliers in the data and helps capture the variations of the language-usage-patterns within a text sample. Specifically, we represent each text sample as a point set and identify the top-k stylistically similar text samples (SSTs) from the corpus. We then apply the probabilistic k nearest neighbors’ classifier on the identified top-k SSTs to predict the native languages of the authors. To conduct experiments, we create three new corpora where each corpus is written in a different language, namely, English, French, and German. Our experimental studies show that our solution outperforms competitive methods and reports more than 80\% accuracy across languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {55},
numpages = {19},
keywords = {text classification, stylometry, native language identification, Author profiling, forensic investigation}
}

@article{Bangalore,
author = {Bangalore, Srinivas and Joshi, Aravind K.},
title = {Supertagging: An Approach to Almost Parsing},
year = {1999},
issue_date = {June 1999},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {25},
number = {2},
issn = {0891-2017},
abstract = {In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context. The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag. Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear. This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework. The supertags in LTAG combine both phrase structure information and dependency information in a single representation. Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need "only" combine the individual supertags. This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure.},
journal = {Comput. Linguist.},
month = {jun},
pages = {237–265},
numpages = {29}
}

@inproceedings{bai,
    title = "Syntax-{BERT}: Improving Pre-trained Transformers with Syntax Trees",
    author = "Bai, Jiangang  and
      Wang, Yujing  and
      Chen, Yiren  and
      Yang, Yaming  and
      Bai, Jing  and
      Yu, Jing  and
      Tong, Yunhai",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.262",
    doi = "10.18653/v1/2021.eacl-main.262",
    pages = "3011--3020",
    abstract = "Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5.",
}

@inproceedings{koehn,
    title = "{E}uroparl: A Parallel Corpus for Statistical Machine Translation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.11",
    pages = "79--86",
    abstract = "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
}

@article{lado1957linguistics,
  title={Linguistics across cultures; applied linguistics for language teachers},
  author={Lado, Robert},
  year={1957}
}

@misc{liu2023code,
      title={Code Execution with Pre-trained Language Models}, 
      author={Chenxiao Liu and Shuai Lu and Weizhu Chen and Daxin Jiang and Alexey Svyatkovskiy and Shengyu Fu and Neel Sundaresan and Nan Duan},
      year={2023},
      eprint={2305.05383},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
url={https://aclanthology.org/2023.findings-acl.308.pdf}
}

@misc{wang2023codet5,
      title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation}, 
      author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui and Junnan Li and Steven C. H. Hoi},
      year={2023},
      eprint={2305.07922},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
url={https://arxiv.org/abs/2305.07922}
}
@inproceedings{han-etal-2019-joint,
    title = "Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction",
    author = "Han, Rujun  and
      Ning, Qiang  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1041",
    doi = "10.18653/v1/D19-1041",
    pages = "434--444",
    abstract = "We propose a joint event and temporal relation extraction model with shared representation learning and structured prediction. The proposed method has two advantages over existing work. First, it improves event representation by allowing the event and relation modules to share the same contextualized embeddings and neural representation learner. Second, it avoids error propagation in the conventional pipeline systems by leveraging structured inference and learning methods to assign both the event labels and the temporal relation labels jointly. Experiments show that the proposed method can improve both event extraction and temporal relation extraction over state-of-the-art systems, with the end-to-end F1 improved by 10{\%} and 6.8{\%} on two benchmark datasets respectively.",


}